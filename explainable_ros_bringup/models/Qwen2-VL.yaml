use_llava: True

n_ctx: 4096
n_batch: 512
n_gpu_layers: 29
n_threads: -1
n_predict: 4096

model_repo: "ggml-org/Qwen2.5-VL-7B-Instruct-GGUF"
model_filename: "Qwen2.5-VL-7B-Instruct-Q4_K_M.gguf"

mmproj_repo: "ggml-org/Qwen2.5-VL-7B-Instruct-GGUF"
mmproj_filename: "mmproj-Qwen2.5-VL-7B-Instruct-f16.gguf"
